import os
from crewai.tools import tool
import google.generativeai as genai
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
import json

token = "AIzaSyDpcS2ySvzyHoGdK1zfJ3rYynWIBTrNAtY"

genai.configure(api_key=token)
model = genai.GenerativeModel("gemini-1.5-flash")


# Load an Arabic-compatible embedding model
model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
model_kwargs = {"device": "cuda"}  # Use CUDA if available

embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)


# โ ูุชุบูุฑุงุช Global ูุชุฎุฒูู ุจูุงูุงุช ุงููุณุชุฎุฏู
USER_AGE = None
USER_WEIGHT = None
USER_HEIGHT = None
USER_STATUS = None

def update_user_info(age, weight, height, status):
    """ุชุญุฏูุซ ุจูุงูุงุช ุงููุณุชุฎุฏู ุนูุฏ ุงุฎุชูุงุฑูุง ูู ูุงุฌูุฉ ุงููุณุชุฎุฏู."""
    global USER_AGE, USER_WEIGHT, USER_HEIGHT, USER_STATUS
    USER_AGE = age
    USER_WEIGHT = weight
    USER_HEIGHT = height
    USER_STATUS = status
    print(f"โ ุชู ุชุญุฏูุซ ุจูุงูุงุช ุงููุณุชุฎุฏู: ุงูุนูุฑ={USER_AGE}, ุงููุฒู={USER_WEIGHT}, ุงูุทูู={USER_HEIGHT}, ุงูุญุงูุฉ ุงูุตุญูุฉ={USER_STATUS}")


def encode_pdfs(paths, chunk_size=2000, chunk_overlap=200):
    """
    Load and encode multiple PDF files into a single FAISS vector store.
    
    :param paths: List of PDF file paths.
    :param chunk_size: Size of text chunks.
    :param chunk_overlap: Overlapping size between chunks.
    :return: FAISS vector store containing indexed document embeddings.
    """
    all_texts = []
    
    for path in paths:
        # Load the PDF
        loader = PyPDFLoader(path)
        documents = loader.load()
        
        # Split the text into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len
        )
        chunks = text_splitter.split_documents(documents)
        
        # Extract text content
        texts = [chunk.page_content.encode("utf-8", "ignore").decode() for chunk in chunks]
        all_texts.extend(texts)  # Collect all texts from different books

    # Create a single FAISS vector store from all documents
    vectorstore = FAISS.from_texts(all_texts, embeddings)
    
    return vectorstore

# Paths of the three books
pdf_paths = [r'books\\Book1.pdf', r'books\\Book2.pdf']

# Encode all books into a single vector store
chunks_vector_store = encode_pdfs(pdf_paths, chunk_size=2000, chunk_overlap=200)

# Create a retriever from the combined vector store
chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={"k": 7})


@tool
def run_rag(prompt: str) -> str:
    """๐ ูุณุชุฑุฌุน ุงููุนูููุงุช ุงูุบุฐุงุฆูุฉ ุจูุงุกู ุนูู ุจูุงูุงุช ุงููุณุชุฎุฏู ุงููุฎุฒูุฉ ูู `Global Variables`."""
    global USER_AGE, USER_WEIGHT, USER_HEIGHT, USER_STATUS

    # โ ุงูุชุฃูุฏ ูู ุฃู ุงูููู ุบูุฑ ูุงุฑุบุฉ ูุจู ุชุดุบูู ุงููููุฐุฌ
    if None in (USER_AGE, USER_WEIGHT, USER_HEIGHT, USER_STATUS):
        return "โ๏ธ ูุฑุฌู ุชุญุฏูุฏ ุจูุงูุงุช ุงููุณุชุฎุฏู (ุงูุนูุฑุ ุงููุฒูุ ุงูุทููุ ุงูุญุงูุฉ ุงูุตุญูุฉ) ูุจู ุงูุงุณุชูุณุงุฑ."

    print(f"๐ฅ Received in run_rag: prompt={prompt}, age={USER_AGE}, weight={USER_WEIGHT}, height={USER_HEIGHT}, status={USER_STATUS}")  

    results = chunks_query_retriever.get_relevant_documents(prompt)

    formatted_prompt = f""" 
    ุฃูุช ูุณุงุนุฏ ุบุฐุงุฆู ุฐููุ ูููุชู ุชูุฏูู ูุธุงู ุบุฐุงุฆู ุตุญู ููุถุญ ููู ุงูุณุนุฑุงุช ุงูุญุฑุงุฑูุฉ ุงูููุงุณุจุฉ ูููุณุชุฎุฏู ุจูุงุกู ุนูู ุงูุจูุงูุงุช ุงูุดุฎุตูุฉ ุงูุชุงููุฉ:
    
    ๐ง ุงูุนูุฑ: {USER_AGE} ุณูุฉ  
    โ๏ธ ุงููุฒู: {USER_WEIGHT} ูุฌู  
    ๐ ุงูุทูู: {USER_HEIGHT} ุณู 
    ๐ฅ ุงูุญุงูุฉ ุงูุตุญูุฉ: {USER_STATUS} 
            
    ๐น ุถุน ูู ุงุนุชุจุงุฑู ุชูุงุฒู ุงูุนูุงุตุฑ ุงูุบุฐุงุฆูุฉุ ููุฏู ุงูุชุฑุงุญูุง ูุญุชูู ุนูู ุงูุจุฑูุชููุ ุงููุฑุจูููุฏุฑุงุชุ ูุงูุฏููู ุงูุตุญูุฉ.  
    ๐น ุฅุฐุง ูุงูุช ุงููุฌุจุฉ ุบูุฑ ููุงุณุจุฉ ูุตุญุฉ ุงููุณุชุฎุฏูุ ุงูุชุฑุญ ุจุฏููุงู ุตุญููุง ูุดุงุจููุง.  
    ๐น ุงุฌุนู ุงูุฅุฌุงุจุฉ ูุงุถุญุฉ ูููุฌุฒุฉ ูู **ุฎูุณ ุฌูู** ููุท.  

    ๐ ูุนูููุงุช ูุฑุฌุนูุฉ:  
    {results}

    ๐ฝ๏ธ ุงูุชุฑุงุญ ูุฌุจุฉ ุตุญูุฉ:
    """
    response = model.generate_content(formatted_prompt)
    return response.text

@tool
def run_general_nutrition_query(prompt: str) -> str:
    """๐ ูุณุชุฑุฌุน ูุนูููุงุช ุบุฐุงุฆูุฉ ุนุงูุฉ ุญูู ุงูุฃุทุนูุฉ ูุงูุชุบุฐูุฉ ุจูุงุกู ุนูู ุงุณุชูุณุงุฑุงุช ุงููุณุชุฎุฏู."""
    
    print(f"๐ฅ Received in run_general_nutrition_query: prompt={prompt}")  

    # ๐น ุงุณุชุฑุฌุงุน ุงูุจูุงูุงุช ุฐุงุช ุงูุตูุฉ ูู ูุธุงู ุงูุจุญุซ
    results = chunks_query_retriever.get_relevant_documents(prompt)

    # ๐น ุตูุงุบุฉ ุงุณุชุนูุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุฅูุชุงุฌ ุฅุฌุงุจุฉ ูููุฏุฉ
    formatted_prompt = f""" 
    ุฃูุช ุฎุจูุฑ ุชุบุฐูุฉ ุฐููุ ูููุชู ูู ุงูุฅุฌุงุจุฉ ุนู ุงูุฃุณุฆูุฉ ุงูุนุงูุฉ ุญูู ุงูุฃุทุนูุฉ ูุงูุชุบุฐูุฉ.
    
    โ ุงุณุชูุณุงุฑ ุงููุณุชุฎุฏู: **{prompt}**  
    โ ูุฏู ุฅุฌุงุจุฉ ุฏูููุฉ ููุฎุชุตุฑุฉ ูุฏุนููุฉ ุจูุนูููุงุช ุนูููุฉ.
    โ ุงุฌุนู ุงูุฅุฌุงุจุฉ ูู **ุฎูุณ ุฌูู ููุท** ุฏูู ุชูุฏูู ุชูุตูุงุช ุทุจูุฉ.

    ๐ ูุนูููุงุช ูุฑุฌุนูุฉ:  
    {results}

    ๐ข **ุฅุฌุงุจุฉ ูุฎุชุตุฑุฉ:**  
    """
    
    response = model.generate_content(formatted_prompt)
    return response.text



@tool 
def load_json_file(file_path="data.json"):
    """
    ูููู ุจุชุญููู ุจูุงูุงุช ูู ููู JSON ูุฅุฑุฌุงุนูุง ูุณูุณูุฉ ูุตูุฉ ููุณูุฉ.
    
    :param file_path: ูุณุงุฑ ููู JSON.
    :return: ูุญุชูู JSON ูุณูุณูุฉ ูุตูุฉ.
    """
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            json_data = json.load(file)  # โ ูุฑุงุกุฉ JSON
            return json.dumps(json_data, indent=4, ensure_ascii=False)  # โ ุชุญููู JSON ุฅูู ูุต ููุณู
    except FileNotFoundError:
        return "โ๏ธ ุงูููู ุบูุฑ ููุฌูุฏ."
    except json.JSONDecodeError:
        return "โ๏ธ ุฎุทุฃ ูู ุชุญููู ุจูุงูุงุช JSON."


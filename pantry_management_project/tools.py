import os
from crewai.tools import tool
from crewai_tools import PDFSearchTool
import google.generativeai as genai
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
import json

token = "AIzaSyDpcS2ySvzyHoGdK1zfJ3rYynWIBTrNAtY"

genai.configure(api_key=token)
model = genai.GenerativeModel("gemini-1.5-flash")


# Load an Arabic-compatible embedding model
model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
model_kwargs = {"device": "cuda"}  # Use CUDA if available

embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)

def encode_pdfs(paths, chunk_size=2000, chunk_overlap=200):
    """
    Load and encode multiple PDF files into a single FAISS vector store.
    
    :param paths: List of PDF file paths.
    :param chunk_size: Size of text chunks.
    :param chunk_overlap: Overlapping size between chunks.
    :return: FAISS vector store containing indexed document embeddings.
    """
    all_texts = []
    
    for path in paths:
        # Load the PDF
        loader = PyPDFLoader(path)
        documents = loader.load()
        
        # Split the text into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len
        )
        chunks = text_splitter.split_documents(documents)
        
        # Extract text content
        texts = [chunk.page_content.encode("utf-8", "ignore").decode() for chunk in chunks]
        all_texts.extend(texts)  # Collect all texts from different books

    # Create a single FAISS vector store from all documents
    vectorstore = FAISS.from_texts(all_texts, embeddings)
    
    return vectorstore

# Paths of the three books
pdf_paths = [r'books\\Book1.pdf', r'books\\Book2.pdf']

# Encode all books into a single vector store
chunks_vector_store = encode_pdfs(pdf_paths, chunk_size=2000, chunk_overlap=200)

# Create a retriever from the combined vector store
chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={"k": 5})

def prompt_template(context: str) -> str:
    prompt = f""" 
    ุฃูุช ูุณุงุนุฏ ุบุฐุงุฆู ุฐููุ ูููุชู ุชูุฏูู ูุธุงู ุบุฐุงุฆู ุตุญู ููุถุญ ููู ุงูุณุนุฑุงุช ุงูุญุฑุงุฑูุฉุงูููุงุณุจุฉ ูููุณุชุฎุฏู ุจูุงุกู ุนูู ุนูุฑ ุงููุณุชุฎุฏูุ ูุฒููุ ููู ุงููููู ุงู ูููู ูู ุชุงุฑูุฎ ูุฑุถู ูู ุงูุจุฏุงูุฉ ุงุทูุจ ููู ูุฐู ุงููุนูููุงุช ุญุชู ุชูุฏู ูุชุงุฆุฌ ูุนุงูุฉ.
    ุงุฐุง ุฑูุถ ุงุนุทุงุก ุงูููุนููุงุช ุงูุดุฎุตูุฉ ููููู ุงู ุชูุฏู ูุตุงุฆุญ ุนุงูุฉ ููุชุบุฐูุฉ ุงูุตุญูุฉ.
    ูุฐุง ุงุนุทุงู ุงููุนูููุงุช ุงูุดุฎุตูุฉ ููููู ุงู ุชูุฏู ูุตุงุฆุญ ุชุบุฐูุฉ ูุฎุตุตุฉ ูู.
    ๐น ุถุน ูู ุงุนุชุจุงุฑู ุชูุงุฒู ุงูุนูุงุตุฑ ุงูุบุฐุงุฆูุฉุ ููุฏู ุงูุชุฑุงุญูุง ูุญุชูู ุนูู ุงูุจุฑูุชููุ ุงููุฑุจูููุฏุฑุงุชุ ูุงูุฏููู ุงูุตุญูุฉ.
    ๐น ุฅุฐุง ูุงูุช ุงููุฌุจุฉ ุบูุฑ ููุงุณุจุฉ ูุตุญุฉ ุงููุณุชุฎุฏูุ ุงูุชุฑุญ ุจุฏููุงู ุตุญููุง ูุดุงุจููุง.
    ๐น ุงุฌุนู ุงูุฅุฌุงุจุฉ ูุงุถุญุฉ ูููุฌุฒุฉ ูู **ุฎูุณ ุฌูู** ููุท.

    ๐ ูุนูููุงุช ูุฑุฌุนูุฉ:
    {context}

    ๐ฝ๏ธ ุงูุชุฑุงุญ ูุฌุจุฉ ุตุญูุฉ:
    """
    return prompt

@tool
def run_rag(prompet: str)->str:
  """๐ ูุณุชุฑุฌุน ุงููุนูููุงุช ูู PDF ููุณุชุฎุฏู Gemini 1.5 ููุฅุฌุงุจุฉ ุนูู ุงุณุชูุณุงุฑุงุช ุงููุณุชุฎุฏู.

    โ ูุชู ุงูุจุญุซ ูู ุงููุณุชูุฏุงุช ุจุงุณุชุฎุฏุงู `FAISS` ูุงุณุชุฑุฌุงุน ุงููุนูููุงุช ุงูุบุฐุงุฆูุฉ ุงููุฑุชุจุทุฉ ุจุงูุณุคุงูุ
    ุซู ูุชู ุชูุฑูุฑ ุงูุจูุงูุงุช ุฅูู `Gemini 1.5` ูุฅูุดุงุก ุฅุฌุงุจุฉ ูุฎุตุตุฉ.

    ๐น **ุงููุนุทูุงุช**:
    - `query` (str): ุงุณุชูุณุงุฑ ุงููุณุชุฎุฏู.

    ๐น **ุงููุฎุฑุฌุงุช**:
    - (str): ุฅุฌุงุจุฉ ุชุญุชูู ุนูู ูุธุงู ุบุฐุงุฆู ุงู ุชูุตูุฉ ุบุฐุงุฆูุฉ ูุจููุฉ ุนูู ุงููุนูููุงุช ุงููุณุชุฑุฌุนุฉ ูู ุงููุณุชูุฏุงุช.
    """
  results = chunks_query_retriever.get_relevant_documents(prompet)
  response = model.generate_content(prompt_template(results))
  return response.text


@tool 
def load_json_file(file_path="data.json"):
    """
    ูููู ุจุชุญููู ุจูุงูุงุช ูู ููู JSON ูุฅุฑุฌุงุนูุง ูุณูุณูุฉ ูุตูุฉ ููุณูุฉ.
    
    :param file_path: ูุณุงุฑ ููู JSON.
    :return: ูุญุชูู JSON ูุณูุณูุฉ ูุตูุฉ.
    """
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            json_data = json.load(file)  # โ ูุฑุงุกุฉ JSON
            return json.dumps(json_data, indent=4, ensure_ascii=False)  # โ ุชุญููู JSON ุฅูู ูุต ููุณู
    except FileNotFoundError:
        return "โ๏ธ ุงูููู ุบูุฑ ููุฌูุฏ."
    except json.JSONDecodeError:
        return "โ๏ธ ุฎุทุฃ ูู ุชุญููู ุจูุงูุงุช JSON."

